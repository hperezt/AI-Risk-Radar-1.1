# app/risk_engine.py
import os
import json
from dotenv import load_dotenv
import openai  
print("DEBUG 췅 openai version:", openai.__version__)
load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
API_KEY = os.getenv("OPENAI_API_KEY")
USE_MOCK = False

if not API_KEY:
    raise RuntimeError("OPENAI_API_KEY no est치 definida. A침치dela en Render > Environment")

openai.api_key = API_KEY  # 游녣 cambio aqu칤


SYSTEM = """
Comentario: Este es un ejercicio de an치lisis asistido por inteligencia artificial. El objetivo es evaluar c칩mo un modelo LLM puede colaborar con expertos humanos para identificar riesgos relevantes en proyectos ferroviarios en Alemania, tanto obvios como sist칠micos. El resultado ser치 revisado por profesionales humanos, por lo tanto, la calidad, claridad y solidez del razonamiento es m치s importante que la cantidad de resultados.

Act칰as como un comit칠 interdisciplinario compuesto por:
- Ingenieros especializados en planificaci칩n y ejecuci칩n de proyectos de infraestructura ferroviaria en Europa.
- Abogados expertos en derecho de infraestructura y normativa aplicable en Alemania.
- Consultores y analistas con experiencia en evaluaci칩n de riesgos en el sector ferroviario alem치n.

Piensa como si estos perfiles discutieran en conjunto cada riesgo y llegaran a un consenso argumentado.

Tu tarea es leer un documento t칠cnico relacionado con un proyecto ferroviario y detectar *riesgos de planificaci칩n* que puedan generar retrasos, sobrecostos, conflictos contractuales o fallas operativas relevantes.

Devuelve un JSON con exactamente dos listas:
- "intuitive_risks": riesgos t칤picos, previsibles y esperables para equipos experimentados.
- "counterintuitive_risks": riesgos inusuales, sist칠micos, interdisciplinares o dif칤ciles de anticipar.

Cada entrada debe tener esta estructura:
{
  "risk": "...",
  "justification": "...",
  "countermeasure": "...",
  "page": 42,
  "evidence": "Extracto del texto que sirvi칩 de base"
}
"""

def generate_risks(text: str, context: str = "", lang: str = "es") -> dict:
    if USE_MOCK:
        raise RuntimeError("USE_MOCK=True pero el modo estricto est치 activo.")
    if not API_KEY:
        raise RuntimeError("OPENAI_API_KEY no est치 definida. A침치dela en .env")
    if not MODEL_NAME:
        raise RuntimeError("MODEL_NAME no est치 definida")

    user_prompt = f"""
Idioma de salida: {lang}

Analiza el documento de un proyecto de infraestructura y genera:
- 5 riesgos intuitivos
- 5 riesgos contraintuitivos

Para cada riesgo, devuelve un JSON con las siguientes claves:
- "risk"
- "justification"
- "countermeasure"
- "page"
- "evidence"

Contexto adicional (si aplica):
{context}

Documento (truncado a 18000 caracteres):
{text[:18000]}

Devuelve solo un JSON v치lido con estas claves:
- "intuitive_risks": lista de 5 objetos
- "counterintuitive_risks": lista de 5 objetos
"""

    response = openai.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3,
        max_tokens=3000,
        response_format={"type": "json_object"}
    )

    try:
        data = json.loads(response.choices[0].message.content)
    except Exception as e:
        raise RuntimeError(f"No se pudo parsear la respuesta como JSON: {e}")

    if not isinstance(data.get("intuitive_risks"), list) or not isinstance(data.get("counterintuitive_risks"), list):
        raise ValueError("El modelo no devolvi칩 el JSON esperado.")

    # Validaci칩n r치pida por estructura m칤nima
    for block in data["intuitive_risks"] + data["counterintuitive_risks"]:
        if not all(k in block for k in ["risk", "justification", "countermeasure", "page", "evidence"]):
            raise ValueError("Falta una de las claves requeridas en un riesgo")

    data["source"] = "openai"
    return data